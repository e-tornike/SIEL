{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import math\n",
    "\n",
    "\n",
    "line_cycler   = (cycler(color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]) +\n",
    "                 cycler(linestyle=[\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\"]))\n",
    "\n",
    "marker_cycler = (cycler(color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]) +\n",
    "                 cycler(linestyle=[\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\"]) +\n",
    "                 cycler(marker=[\"4\", \"2\", \"3\", \"1\", \"+\", \"x\", \".\"]))\n",
    "\n",
    "plt.rc(\"axes\", prop_cycle=line_cycler)\n",
    "plt.rc(\"savefig\", dpi=200)\n",
    "plt.rc(\"legend\", loc=\"best\", fontsize=\"medium\", fancybox=True, framealpha=0.5)\n",
    "plt.rc(\"lines\", linewidth=2.5, markersize=10, markeredgewidth=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_json(path: str = \"\") -> dict:\n",
    "    assert os.path.isfile(path)\n",
    "    with open(path, \"r\") as infile:\n",
    "        data = json.load(infile)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with jsonlines.open(path, \"r\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_best_row(df, metric):\n",
    "    best_idx = df[metric].idxmax()\n",
    "    best_row = df.loc[best_idx].to_dict()  # this should be loc and not iloc\n",
    "    return best_row\n",
    "\n",
    "\n",
    "def normalize_metric_name(metric):\n",
    "    name = metric.split(\"Binary\")[-1].split(\"()\")[0]\n",
    "    if \":\" in metric:\n",
    "        lang = metric.split(\":\")[1].upper()\n",
    "        name = name+\" \"+lang\n",
    "\n",
    "    name = name.replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")\n",
    "    return name\n",
    "\n",
    "\n",
    "def rename_model(name):\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    \n",
    "    name = name.lower()\n",
    "\n",
    "    if \"cross-en-de-roberta-sentence-transformer\" in name:\n",
    "        name = \"Cross$_{en-de}$\"\n",
    "    elif \"xlm-r-distilroberta\" in name:\n",
    "        name = \"Para\"\n",
    "        # name = \"XLM-R$_{distil}$\"\n",
    "    elif \"simcse-xlmrb-ssoar-noprefix\" in name:\n",
    "        name = \"SoSSe-XLM-R$_{noprefix}$\"\n",
    "    elif \"simcse-xlmrb-ssoar-prefix\" in name:\n",
    "        name = \"SoSSe-XLM-R$_{prefix}$\"\n",
    "    elif \"specter\" in name:\n",
    "        name = \"SPECTER\"\n",
    "    elif \"bert-base-multilingual\" in name:\n",
    "        if \"uncased\" in name:\n",
    "            name = \"mBERT-unca\"\n",
    "        elif \"cased\" in name:\n",
    "            name = \"mBERT-ca\"\n",
    "        else:\n",
    "            name = \"mBERT\"\n",
    "    elif \"distilbert\" in name:\n",
    "        name = \"DistilBERT\"\n",
    "    elif \"multilingual-e5-small\" in name:\n",
    "        name = \"mE5$_{small}$\"\n",
    "    elif \"multilingual-e5-base\" in name:\n",
    "        name = \"mE5$_{base}$\"\n",
    "    elif \"multilingual-e5-large\" in name:\n",
    "        name = \"mE5$_{large}$\"\n",
    "    elif \"e5-large\" in name:\n",
    "        name = \"E5$_{large}$\"\n",
    "    elif \"e5-base\" in name:\n",
    "        name = \"E5$_{base}$\"\n",
    "    elif \"multilingual-minilm\" in name:\n",
    "        name = \"mMiniLM\" \n",
    "    elif \"minilm\" in name:\n",
    "        name = \"MiniLM\"\n",
    "    elif \"multilingual-mpnet\" in name:\n",
    "        name = \"mMPNet\"\n",
    "    elif \"t5\" in name:\n",
    "        name = \"Sentence-T5\"\n",
    "    elif \"ssci\" in name:\n",
    "        name = \"SsciBERT\"\n",
    "    elif \"scibert\" in name:\n",
    "        name = \"SciBERT\"\n",
    "    elif \"distilbert\" in name:\n",
    "        name = \"DistilBERT\"\n",
    "    elif \"xlm-roberta-base\" in name:\n",
    "        name = \"XLM-R$_{\\text{base}}$\"\n",
    "    elif \"xlm-roberta-large\" in name:\n",
    "        name = \"XLM-R$_{\\text{large}}$\"\n",
    "    elif \"xlm-roberta-large\" in name:\n",
    "        name = \"XLM-R$_{\\text{large}}$\"\n",
    "    elif \"xlm-v-base\" in name:\n",
    "        name = \"XLM-V$_{\\text{base}}$\"\n",
    "    elif \"mdeberta-v3-base\" in name:\n",
    "        name = \"mDeBERTa$_{\\text{base}}$\"\n",
    "    elif \"deberta-v3-base\" in name:\n",
    "        name = \"DeBERTa$_{\\text{base}}$\"\n",
    "    elif \"deberta-v3-large\" in name:\n",
    "        name = \"DeBERTa$_{\\text{large}}$\"\n",
    "    elif \"roberta-base\" in name:\n",
    "        name = \"RoBERTa$_{\\text{base}}$\"\n",
    "    elif \"roberta-large\" in name:\n",
    "        name = \"RoBERTa$_{\\text{large}}$\"\n",
    "    elif \"xlmr_large\" in name:\n",
    "        name = \"SoSci-XLM-R$_{\\text{large}}$\"\n",
    "    elif \"xlmr_steps\" in name:\n",
    "        name = \"SoSci-XLM-R$_{\\text{base}}$\"\n",
    "    elif \"mbert_steps\" in name:\n",
    "        name = \"SoSci-mBERT\"\n",
    "    elif \"bert-base\" in name:\n",
    "        name = \"BERT$_{\\text{base}}$\"\n",
    "    elif \"bert-large\" in name:\n",
    "        name = \"BERT$_{\\text{large}}$\"\n",
    "    elif \"logisticregression\" in name:\n",
    "        name = \"LR\"\n",
    "    elif \"linearsvm\" in name or \"{linear}\" in name:\n",
    "        name = \"SVM$_{linear}$\"\n",
    "    elif \"svm\" in name or \"{non-linear}\" in name:\n",
    "        name = \"SVM$_{non-linear}$\"\n",
    "    elif \"mistral-7b\" in name:\n",
    "        name = \"Mistral-7B\"\n",
    "    elif \"mixtral-8x7b\" in name:\n",
    "        name = \"Mixtral-8x7B\"\n",
    "    else:\n",
    "        print(\"No matching rules for model:\", name)\n",
    "        pass\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def rename_metric(name):\n",
    "    if \"recall\" in name:\n",
    "        name = name.replace(\"recall\", \"r\")\n",
    "    elif \"map\" in name:\n",
    "        name = name.replace(\"map\", \"MAP\")\n",
    "    elif \"ndcg\" in name:\n",
    "        name = name.replace(\"ndcg\", \"nDCG\")\n",
    "    else:\n",
    "        print(\"No matching rules for metric:\", name)\n",
    "        pass\n",
    "    \n",
    "    return name\n",
    "\n",
    "\n",
    "def bold_str(s):\n",
    "    return \"\\\\textbf{\"+s+\"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "runs_dir: str = \"../results/runs_sv4_journal_paper_results\"\n",
    "test_path: str = \"../data/sild/diff_test.tsv\"\n",
    "save_results: bool = True\n",
    "group_output: bool = False\n",
    "groupings: List[str] = [\"cmodel\", \"rmodel\"]\n",
    "sortings: str = \"weight\"\n",
    "plot_figures: bool = False\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\"]\n",
    "max_metric: str = \"test_lang:en:BinaryF1Score()_mean\"\n",
    "n_columns: int = 2\n",
    "fontsize: float = 18.0\n",
    "figure_height: int = 30\n",
    "figure_width: int = 30\n",
    "font: str = \"serif\"\n",
    "save_tables: bool = True\n",
    "table_1: bool = False\n",
    "table_2: bool = False\n",
    "table_3: bool = False\n",
    "table_35: bool = False\n",
    "table_4: bool = False\n",
    "\n",
    "output_dir: str = \"../results/results_15-03-2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/31 [00:00<00:01, 18.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/cluster-2024-03-26_16-28\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval-2024-03-13_13-41_10\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_contextwindow-2024-04-20_20-53\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic-2024-03-11_09-20_100k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/31 [00:00<00:02, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval_mixtral-2024-03-13_13-39_0\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/cluster_tfidf-2024-05-06_15-52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 8/31 [00:00<00:01, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval-2024-03-13_13-41_0\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/rac-2024-03-27\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval_mixtral-2024-03-13_13-39_10\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic-2024-03-11_09-21_400k-800k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10/31 [00:00<00:01, 14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/linear-2024-05-17_10-09\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_contextwindow-2024-05-06_17-20\n",
      "Results file does not exist: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_contextwindow-2024-05-06_17-20/FacebookAI--xlm-roberta-large-finetuned_do-retrieval=False_20240510-091909/results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 14/31 [00:01<00:01, 12.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic_tentrials-2024-05-13_21-22\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval-2024-03-13_13-41_50\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic-2024-03-29_best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 18/31 [00:01<00:00, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_diff_vs_rand-2024-05-06_23-04\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/cluster_sim-2024-04-19_08-58\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval_cp-2024-03-11_21-27\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic-2024-03-11_09-26_1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 20/31 [00:01<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic_fast_v2-2024-05-13_22-55\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval-2024-03-13_13-41_100\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/test_contextwindows-2024-05-13_16-57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 24/31 [00:01<00:00, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic_best_fast-2024-05-08_20-52\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic_fast-2024-05-13_19-19\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval_mixtral-2024-03-13_13-39_50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 26/31 [00:02<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval_mixtral-2024-03-13_13-39_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 28/31 [00:02<00:00, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic_fast_v3-2024-05-14_00-44\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/llmeval-2024-03-13_13-41_20\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/_linear-2024-03-02_10-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 30/31 [00:02<00:00, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune_synthetic-2024-04-22_08-18\n",
      "Working on results dir: ../results/runs_sv4_journal_paper_results_14-03-2024/finetune-2024-03-11_11-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:02<00:00, 11.90it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dirs = [os.path.join(runs_dir, d) for d in os.listdir(runs_dir)]\n",
    "\n",
    "full_df = pd.DataFrame()\n",
    "full_raw_results = []\n",
    "    \n",
    "for results_dir in tqdm(results_dirs):\n",
    "    print(f\"Working on results dir: {results_dir}\")\n",
    "    results_dir_type = os.path.basename(results_dir).split(\"-\")[0]\n",
    "\n",
    "    if os.path.isfile(results_dir):\n",
    "        continue\n",
    "\n",
    "    model_dirs = [os.path.join(results_dir, d) for d in os.listdir(results_dir)]\n",
    "\n",
    "    if test_path:\n",
    "        test_df = pd.read_csv(test_path, sep=\"\\t\").rename(columns={\"sentence\": \"text\", \"is_variable\": \"label\"})\n",
    "        test_df.index = test_df[\"uuid\"]\n",
    "\n",
    "    full_results = []\n",
    "\n",
    "    for model_dir in model_dirs:\n",
    "        if os.path.isfile(model_dir) or \"figure\" in model_dir or \"tables\" in model_dir:\n",
    "            continue\n",
    "\n",
    "        if \"knn\" in model_dir:\n",
    "            algorithm_name = \"KNN\"\n",
    "        elif \"rnn\" in model_dir:\n",
    "            algorithm_name = \"RNN\"\n",
    "        elif \"kmeans\" in model_dir:\n",
    "            algorithm_name = \"KMeans\"\n",
    "        elif \"logisticregression\" in model_dir:\n",
    "            algorithm_name = \"LR\"\n",
    "        elif \"linearsvm\" in model_dir:\n",
    "            algorithm_name = \"SVM$_{linear}$\"\n",
    "        elif \"svm\" in model_dir:\n",
    "            algorithm_name = \"SVM$_{non-linear}$\"\n",
    "        elif \"mistral\" in model_dir:\n",
    "            algorithm_name = \"Mistral\"\n",
    "        else:\n",
    "            algorithm_name = \"Unk\"\n",
    "        \n",
    "        avg_results = {}\n",
    "        raw_results = {}\n",
    "\n",
    "        results_path = os.path.join(model_dir, \"results.json\")\n",
    "        if os.path.isfile(results_path):\n",
    "            results = load_json(results_path)\n",
    "\n",
    "            if \"rac\" in results_dir_type:\n",
    "                _row = {\"algorithm\": \"rac\", \"model_dir\": model_dir, \"result_type\": results_dir_type, \"rmodel\": os.path.basename(model_dir)}\n",
    "                for k,v in results.items():\n",
    "                    _row[k] = v[0] if isinstance(v, list) else v\n",
    "                full_results.append(_row)\n",
    "                continue\n",
    "\n",
    "            config_path = os.path.join(model_dir, \"config.json\")\n",
    "            if os.path.isfile(config_path):\n",
    "                config = load_json(config_path)\n",
    "\n",
    "                config_key = list(config.keys())[0]  # each run fold has identical parameters\n",
    "                run_config = {seed: cfg[\"run\"] for seed, cfg in config[config_key].items()}\n",
    "                run_key = list(run_config.keys())[0] # each seed has identical parameters\n",
    "\n",
    "                cmodel = run_config[run_key][\"classification_model_name_or_path\"]\n",
    "                ret_test = run_config[run_key][\"do_retrieval_during_inference\"]\n",
    "            else:\n",
    "                ret_test = \"\"\n",
    "                cmodel = os.path.basename(model_dir)\n",
    "\n",
    "            avg_ret_results = {}\n",
    "            for metric, seed_results in results.items():\n",
    "                if metric not in [\"retriever_results\"]:\n",
    "                    _results = []\n",
    "                    for vals in seed_results.values():\n",
    "                        _results.extend(vals)\n",
    "                    \n",
    "                    avg_results[metric+\"_mean\"] = np.mean(_results)\n",
    "                    avg_results[metric+\"_std\"] = np.std(_results)\n",
    "                    avg_results[metric+\"_raw\"] = _results\n",
    "\n",
    "                    raw_results[metric] = _results\n",
    "                    \n",
    "                elif \":\" in metric:\n",
    "                    pass\n",
    "                else:\n",
    "                    _results = defaultdict(lambda: defaultdict(list))\n",
    "                    for seed, seed_values in seed_results.items():\n",
    "                        for model_name, model_values in seed_values.items():\n",
    "                            for metric, metric_values in model_values.items():\n",
    "                                _results[model_name][metric].extend(metric_values)\n",
    "\n",
    "                    for model_name, metric_values_list in _results.items():\n",
    "                        weight_groups = list(set([k.split(\"weight=\")[-1] for k in list(metric_values_list.keys())]))\n",
    "                        for group_name in weight_groups:\n",
    "                            model_avg_ret_results = deepcopy(avg_ret_results)\n",
    "                            model_avg_ret_results[\"rmodel\"] = model_name\n",
    "                            model_avg_ret_results[\"weight\"] = group_name\n",
    "                            for metric, values in metric_values_list.items():\n",
    "                                if group_name in metric:\n",
    "                                    metric_name = metric.split(group_name)[0].split(\"_weight\")[0]\n",
    "                                    model_avg_ret_results[metric_name+\"_mean\"] = np.mean(values)\n",
    "                                    model_avg_ret_results[metric_name+\"_std\"] = np.std(values)\n",
    "                            full_results.append(model_avg_ret_results)\n",
    "                    \n",
    "            if avg_results:\n",
    "                avg_results[\"retrieval_test\"] = ret_test\n",
    "                avg_results[\"cmodel\"] = cmodel\n",
    "                avg_results[\"model_dir\"] = model_dir\n",
    "                avg_results[\"algorithm\"] = algorithm_name\n",
    "                avg_results[\"result_type\"] = results_dir_type\n",
    "\n",
    "            if raw_results:\n",
    "                raw_results[\"retrieval_test\"] = ret_test\n",
    "                raw_results[\"cmodel\"] = cmodel\n",
    "                raw_results[\"model_dir\"] = model_dir\n",
    "                raw_results[\"algorithm\"] = algorithm_name\n",
    "                raw_results[\"result_type\"] = results_dir_type\n",
    "\n",
    "        else:\n",
    "            if \"tables\" not in results_path or \"figures\" not in results_path:\n",
    "                print(f\"Results file does not exist: {results_path}\")\n",
    "            continue\n",
    "\n",
    "        if avg_results:\n",
    "            full_results.append(avg_results)\n",
    "\n",
    "        if raw_results:\n",
    "            full_raw_results.append(raw_results)\n",
    "\n",
    "        pred_files = [os.path.join(model_dir, f) for f in os.listdir(model_dir) if \"preds\" in f]\n",
    "\n",
    "        if pred_files:\n",
    "            preds = defaultdict(list)\n",
    "            labels = {}\n",
    "            for f in pred_files:\n",
    "                df = pd.read_csv(f, sep=\"\\t\")\n",
    "                for i in range(df.shape[0]):\n",
    "                    row = df.iloc[i]\n",
    "                    uuid = row[\"uuid\"]\n",
    "                    pred = row[\"pred\"]\n",
    "                    label = row[\"label\"]\n",
    "                    preds[uuid].append(str(pred))\n",
    "                    labels[uuid] = label\n",
    "\n",
    "            if test_path:\n",
    "                pred_data = []\n",
    "                for i in range(test_df.shape[0]):\n",
    "                    row = test_df.iloc[i]\n",
    "                    uuid = row[\"uuid\"]\n",
    "                    if uuid in preds:\n",
    "                        assert row[\"label\"] == labels[uuid]\n",
    "                        new_row = {\"uuid\": uuid, \"label\": row[\"label\"], \"preds\": \",\".join(preds[uuid]), \"text\": row[\"text\"]}\n",
    "                        pred_data.append(new_row)\n",
    "\n",
    "                pred_df = pd.DataFrame(pred_data)\n",
    "                output_path = os.path.join(model_dir, \"test_predictions.tsv\")\n",
    "                pred_df.to_csv(output_path, index=False, sep=\"\\t\")\n",
    "        \n",
    "    if save_results:\n",
    "        output_file = os.path.join(results_dir, \"results.tsv\")\n",
    "        \n",
    "        if not full_results:\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(full_results)\n",
    "        full_df = pd.concat([full_df, df])\n",
    "\n",
    "full_df = full_df.reset_index()\n",
    "raw_df = pd.DataFrame(full_raw_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from deepsig import multi_aso\n",
    "\n",
    "\n",
    "def make_stat_sig(df, metric, model_col=\"cmodel\", return_df=False, seed=42, num_jobs=24, confidence_level=0.95, output_path=None, rename_models=True):\n",
    "    all_model_scores = []\n",
    "    models = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        model_scores = df.iloc[i][metric]\n",
    "        all_model_scores.append(model_scores)\n",
    "        if rename_models:\n",
    "            models.append(rename_model(df.iloc[i][model_col]))\n",
    "        else:\n",
    "            models.append(df.iloc[i][model_col])\n",
    "\n",
    "    n_models = len(models)\n",
    "\n",
    "    eps_min = multi_aso(all_model_scores, confidence_level=confidence_level, return_df=return_df, seed=seed, num_jobs=num_jobs)\n",
    "    eps_min = np.concatenate((eps_min, np.array(models).reshape(n_models,1)), axis=1)\n",
    "\n",
    "    df = pd.DataFrame(eps_min, columns=models+[\"Model\"]).set_index(\"Model\")\n",
    "\n",
    "    if output_path:\n",
    "        df.to_latex(output_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_significant_rows(eps_min, p_val=0.2):\n",
    "    significant_rows = []\n",
    "\n",
    "    for i in range(eps_min.shape[0]):\n",
    "        row_k = eps_min.iloc[i].name\n",
    "        max_val = float(max(eps_min.loc[:, eps_min.columns != row_k].iloc[i].values.astype(np.float64)))\n",
    "        if max_val <= p_val:\n",
    "            significant_rows.append((i,row_k))\n",
    "\n",
    "    return significant_rows\n",
    "\n",
    "\n",
    "def mark_significant_rows(df, significant_rows):\n",
    "    for i,_,column in significant_rows:\n",
    "        df.loc[i, column] = df.loc[i, column].replace(\"$^*_\", \"$^{**}_\").replace(\"$_\", \"$^*_\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_significance_to_table(final_df, raw_df, raws, model_col=\"cmodel\", p_val=0.2, output_dir=None, rename_models=True):\n",
    "    significant_rows = []\n",
    "    for metric in raws:\n",
    "        output_path = os.path.join(output_dir, metric+\".tex\") if output_dir else None\n",
    "        eps_min = make_stat_sig(raw_df, metric, model_col=model_col, return_df=True, output_path=output_path, rename_models=rename_models)\n",
    "        srows = find_significant_rows(eps_min, p_val)\n",
    "        srows = [(r[0], r[1], normalize_metric_name(metric)) for r in srows]\n",
    "        significant_rows.extend(srows)\n",
    "\n",
    "    if significant_rows:\n",
    "        final_df = mark_significant_rows(final_df, significant_rows)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def pre_processing(df, metrics, model_cols, filter_criteria):\n",
    "    for crit_name, crit_vals in filter_criteria:\n",
    "        if crit_name in df:\n",
    "            if crit_name in [\"weight\", \"algorithm\"] and crit_vals:\n",
    "                df = df[df[crit_name].isin(crit_vals)]\n",
    "\n",
    "            if crit_name in [\"algorithm\", \"rmodel\"]:\n",
    "                model_cols.append(crit_name)\n",
    "\n",
    "    stds = [m.replace(\"mean\", \"std\") for m in metrics]\n",
    "    raws = [m.replace(\"mean\", \"raw\") for m in metrics]\n",
    "    df = df[model_cols+metrics+stds+raws]\n",
    "\n",
    "    return df, stds, raws\n",
    "\n",
    "\n",
    "def format_scores(df, metrics, stds, cols):\n",
    "    new_rows = []\n",
    "    metric_cols = []\n",
    "    for i in range(df.shape[0]):\n",
    "        row = deepcopy(df.iloc[i])\n",
    "        for m,s in zip(metrics, stds):\n",
    "            name = normalize_metric_name(m)\n",
    "            if name not in metric_cols:\n",
    "                metric_cols.append(name)\n",
    "            rounded_mean = str(round(row[m]*100, 1))\n",
    "            rounded_std = '{:.2f}'.format(round(row[s]*100, 0)).replace('0.', '.').replace('.00', '')\n",
    "            row[name] = rounded_mean+\"$_{\\pm\"+rounded_std+\"}$\"\n",
    "        new_rows.append(row)\n",
    "    cols_to_keep = cols + list(metric_cols)\n",
    "\n",
    "    return new_rows, cols_to_keep\n",
    "\n",
    "\n",
    "def format_table(df, model_order=None, index_col=\"cmodel\", drop_columns=[\"algorithm\"], merge_example_columns=False, bold_table=True):\n",
    "    df[\"cmodel\"] = df[\"cmodel\"].apply(lambda x: rename_model(x))\n",
    "    df = df.rename(columns={index_col: \"Model\"})\n",
    "    df.index = df[\"Model\"]\n",
    "    if model_order:\n",
    "        df = df.loc[[m for m in model_order if m in df.index]]\n",
    "    else:\n",
    "        df = df.sort_values(by=\"Examples\")\n",
    "        if merge_example_columns:\n",
    "            df[\"Model\"] = df.apply(lambda x: str(x.Model)+\"$_{\"+str(x.Examples)+\"}$\", axis=1)\n",
    "        df = df.drop(columns=[\"Examples\"])\n",
    "\n",
    "    if index_col != \"algorithm\":\n",
    "        if df[\"algorithm\"].unique().shape[0] > 1:\n",
    "            df[\"Model\"] = df.apply(lambda x: f\"{x.algorithm}$_{x.Model}$\", axis=1)\n",
    "\n",
    "    df = df.drop(columns=drop_columns)\n",
    "    if bold_table:\n",
    "        df[\"Model\"] = df[\"Model\"].apply(lambda x: bold_str(x))\n",
    "        df = df.rename(bold_str, axis=\"columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def make_table_knn(df, metrics, model_order, keep_weights=None, keep_algorithms=[\"KNN\"], p_val=0.05, output_dir=None):\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    df, stds, raws = pre_processing(df, metrics, model_cols=[\"cmodel\"], filter_criteria=[(\"algorithm\", keep_algorithms), (\"weight\", keep_weights), (\"rmodel\", None)])\n",
    "\n",
    "    new_rows, cols_to_keep = format_scores(df, metrics, stds, [\"cmodel\", \"algorithm\"])\n",
    "    final_df = pd.DataFrame(new_rows)[cols_to_keep].reset_index(drop=True)\n",
    "    raw_df = pd.DataFrame(new_rows)[cols_to_keep+raws].reset_index(drop=True)\n",
    "\n",
    "    final_df = add_significance_to_table(final_df, raw_df, raws, p_val=p_val, output_dir=output_dir)\n",
    "\n",
    "    print(final_df[\"cmodel\"].unique())\n",
    "\n",
    "    final_df = format_table(final_df, model_order)\n",
    "\n",
    "    if output_dir:\n",
    "        final_df.to_latex(os.path.join(output_dir, \"knn.tex\"), index=False, caption=\"Nearest-neighbor classification performance across models for each language using KNN.\")\n",
    "    \n",
    "    return final_df.to_latex(index=False, caption=\"Nearest-neighbor classification performance across models for each language using KNN.\").replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5$_{small}$\", \"mE5$_{small}$\", \"E5$_{base}$\", \"mE5$_{base}$\", \"E5$_{large}$\", \"mE5$_{large}$\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_knn(full_df[full_df[\"result_type\"].isin([\"cluster\", \"cluster_tfidf\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"knn\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5$_{small}$\", \"mE5$_{small}$\", \"E5$_{base}$\", \"mE5$_{base}$\", \"E5$_{large}$\", \"mE5$_{large}$\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_knn(full_df[full_df[\"result_type\"].isin([\"cluster_sim\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"knn-sim\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5$_{small}$\", \"mE5$_{small}$\", \"E5$_{base}$\", \"mE5$_{base}$\", \"E5$_{large}$\", \"mE5$_{large}$\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_knn(full_df[full_df[\"result_type\"].isin([\"finetune_synthetic_best_fast\"])], metrics, model_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5$_{small}$\", \"mE5$_{small}$\", \"E5$_{base}$\", \"mE5$_{base}$\", \"E5$_{large}$\", \"mE5$_{large}$\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_knn(full_df[full_df[\"result_type\"].isin([\"cluster_sim\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"knn\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def make_table_linear(df, metrics, model_order, keep_weights=None, keep_algorithms=[\"LR\", \"SVM$_{linear}$\", \"SVM$_{non-linear}$\"], p_val=0.05, output_dir=None):\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    df, stds, raws = pre_processing(df, metrics, model_cols=[\"cmodel\"], filter_criteria=[(\"algorithm\", keep_algorithms), (\"weight\", keep_weights), (\"rmodel\", None)])\n",
    "\n",
    "    new_rows, cols_to_keep = format_scores(df, metrics, stds, [\"cmodel\", \"algorithm\"])\n",
    "    final_df = pd.DataFrame(new_rows)[cols_to_keep].reset_index(drop=True)\n",
    "    raw_df = pd.DataFrame(new_rows)[cols_to_keep+raws].reset_index(drop=True)\n",
    "\n",
    "    final_df = add_significance_to_table(final_df, raw_df, raws, model_col=\"algorithm\", p_val=p_val, output_dir=output_dir)\n",
    "\n",
    "    final_df = format_table(final_df, model_order, index_col=\"algorithm\", drop_columns=[\"cmodel\"])\n",
    "\n",
    "    if output_dir:\n",
    "        final_df.to_latex(os.path.join(output_dir, \"linear.tex\"), index=False, caption=\"Results for lexical classifiers. LR stands for logistic regression and SVM stands for support vector machine, marked with the subscripts for the linear variant and the non-linear kernel variants.\")\n",
    "    \n",
    "    return final_df.to_latex(index=False, caption=\"Results for lexical classifiers. LR stands for logistic regression and SVM stands for support vector machine, marked with the subscripts for the linear variant and the non-linear kernel variants.\").replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching rules for model: lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model comparisons: 100%|█████████▉| 2997/3000 [00:03<00:00, 925.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching rules for model: lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model comparisons: 100%|█████████▉| 2997/3000 [00:03<00:00, 958.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching rules for model: lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model comparisons: 100%|█████████▉| 2997/3000 [00:03<00:00, 936.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Results for lexical classifiers. LR stands for logistic regression and SVM stands for support vector machine, marked with the subscripts for the linear variant and the non-linear kernel variants.}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "\\textbf{Model} & \\textbf{$P$ EN} & \\textbf{$R$ EN} & \\textbf{$F_1$ EN} \\\\\n",
      "\\midrule\n",
      "\\textbf{LR} & 86.4$^*_{\\pm4}$ & 7.8$_{\\pm4}$ & 14.0$_{\\pm6}$ \\\\\n",
      "\\textbf{SVM$_{linear}$} & 76.7$_{\\pm1}$ & 15.3$_{\\pm1}$ & 25.5$_{\\pm2}$ \\\\\n",
      "\\textbf{SVM$_{non-linear}$} & 66.7$_{\\pm3}$ & 19.3$^*_{\\pm1}$ & 29.9$^*_{\\pm1}$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_order = [\"LR\", \"SVM$_{linear}$\", \"SVM$_{non-linear}$\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\"]\n",
    "print(make_table_linear(full_df[full_df[\"result_type\"].isin([\"linear\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"linear\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def make_table_finetune(df, metrics, model_order, keep_weights=None, keep_algorithms=[\"Unk\"], p_val=0.05, output_dir=None):\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df[\"_model\"] = df[\"cmodel\"].apply(lambda x: rename_model(x))\n",
    "    df = df[df[\"_model\"].isin(model_order)]\n",
    "    df = df.drop(columns=[\"_model\"])\n",
    "        \n",
    "    df, stds, raws = pre_processing(df, metrics, model_cols=[\"cmodel\"], filter_criteria=[(\"algorithm\", keep_algorithms), (\"weight\", keep_weights), (\"rmodel\", None)])\n",
    "\n",
    "    new_rows, cols_to_keep = format_scores(df, metrics, stds, [\"cmodel\", \"algorithm\"])\n",
    "    final_df = pd.DataFrame(new_rows)[cols_to_keep].reset_index(drop=True)\n",
    "    raw_df = pd.DataFrame(new_rows)[cols_to_keep+raws].reset_index(drop=True)\n",
    "\n",
    "    final_df = add_significance_to_table(final_df, raw_df, raws, p_val=p_val, output_dir=output_dir)\n",
    "\n",
    "    final_df = format_table(final_df, model_order)\n",
    "\n",
    "    if output_dir:\n",
    "        final_df.to_latex(os.path.join(output_dir, \"finetune.tex\"), index=False, caption=\"Fine-tuned transformers.\")\n",
    "\n",
    "    return final_df.to_latex(index=False, caption=\"Fine-tuned transformers.\").replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"XLM-R$_{\\text{base}}$\", \"XLM-R$_{\\text{large}}$\", \"XLM-V$_{\\text{base}}$\", \"mDeBERTa$_{\\text{base}}$\", \"DeBERTa$_{\\text{base}}$\", \"DeBERTa$_{\\text{large}}$\", \"SoSci-mBERT\", \"SoSci-XLM-R\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5\", \"mE5\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_finetune(full_df[full_df[\"result_type\"].isin([\"finetune\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"finetune_mono\")))\n",
    "\n",
    "model_order = [\"BERT$_{\\text{base}}$\", \"BERT$_{\\text{large}}$\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"DeBERTa$_{\\text{base}}$\", \"DeBERTa$_{\\text{large}}$\", \"SciBERT\", \"SPECTER\", \"SsciBERT\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_finetune(full_df[full_df[\"result_type\"].isin([\"finetune\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"finetune_ssoar\")))\n",
    "\n",
    "model_order = [\"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"XLM-R$_{\\text{base}}$\", \"XLM-R$_{\\text{large}}$\", \"XLM-V$_{\\text{base}}$\", \"mDeBERTa$_{\\text{base}}$\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_finetune(full_df[full_df[\"result_type\"].isin([\"finetune\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"finetune_multi\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_order = [\"BERT$_{\\text{large}}$\", \"RoBERTa$_{\\text{large}}$\", \"DeBERTa$_{\\text{large}}$\", \"mBERT-unca\", \"XLM-R$_{\\text{base}}$\", \"XLM-R$_{\\text{large}}$\", \"SoSci-mBERT\", \"SoSci-XLM-R$_{\\text{base}}$\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_finetune(full_df[full_df[\"result_type\"].isin([\"finetune\", \"finetune_diff_vs_rand\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"finetune_diff_vs_rand\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"XLM-R\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5\", \"mE5\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_finetune(full_df[full_df[\"algorithm\"].isin([\"Unk\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"finetune_unk\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"XLM-R\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5\", \"mE5\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(full_df[full_df[\"result_type\"].isin([\"test_contextwindows\"])][metrics].apply(lambda x: round(x*100,1)).to_latex(index=False).replace(\"00000\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrrr}\n",
      "\\toprule\n",
      "test_lang:en:BinaryPrecision()_mean & test_lang:en:BinaryRecall()_mean & test_lang:en:BinaryF1Score()_mean & test_lang:de:BinaryPrecision()_mean & test_lang:de:BinaryRecall()_mean & test_lang:de:BinaryF1Score()_mean & test_BinaryF1Score()_mean \\\\\n",
      "\\midrule\n",
      "43.6 & 26.3 & 32.5 & 49.8 & 19.6 & 27.3 & 31.1 \\\\\n",
      "46.2 & 22.2 & 29.5 & 57.2 & 23.0 & 32.2 & 30.3 \\\\\n",
      "20.5 & 10.1 & 13.6 & 22.4 & 10.3 & 14.1 & 13.7 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa$_{\\text{base}}$\", \"RoBERTa$_{\\text{large}}$\", \"XLM-R\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5\", \"mE5\"]\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(full_df[full_df[\"result_type\"].isin([\"finetune_synthetic_best_fast\"])][metrics].apply(lambda x: round(x*100,1)).to_latex(index=False).replace(\"00000\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_icl(df, metrics, keep_weights=None, keep_algorithms=None, p_val=0.05, output_dir=None, bold_table=False):\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    df, stds, raws = pre_processing(df, metrics, model_cols=[\"cmodel\"], filter_criteria=[(\"algorithm\", keep_algorithms), (\"weight\", keep_weights), (\"rmodel\", None)])\n",
    "\n",
    "    new_rows, cols_to_keep = format_scores(df, metrics, stds, [\"cmodel\", \"algorithm\"])\n",
    "    final_df = pd.DataFrame(new_rows)[cols_to_keep].reset_index(drop=True)\n",
    "    raw_df = pd.DataFrame(new_rows)[cols_to_keep+raws].reset_index(drop=True)\n",
    "    \n",
    "    final_df = add_significance_to_table(final_df, raw_df, raws, p_val=p_val, output_dir=output_dir, rename_models=False)\n",
    "\n",
    "    final_df[\"Examples\"] = final_df[\"cmodel\"].apply(lambda x: int(x.split(\"_\")[-1]))\n",
    "    final_df = format_table(final_df, merge_example_columns=True, bold_table=bold_table)\n",
    "\n",
    "    if not bold_table:\n",
    "        final_df[\"Model\"] = final_df[\"Model\"].apply(lambda x: bold_str(x))\n",
    "        final_df = final_df.rename(bold_str, axis=\"columns\")\n",
    "\n",
    "    if output_dir:\n",
    "        final_df.to_latex(os.path.join(output_dir, \"in-context-learning.tex\"), index=False, caption=\"In-context learning results.\")\n",
    "\n",
    "    return final_df.to_latex(index=False, caption=\"In-context learning results.\").replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "print(make_table_icl(full_df[full_df[\"result_type\"].isin([\"llmeval\", \"llmeval_mixtral\"])], metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_rac(df, metrics, model_order=None, keep_algorithms=None, output_dir=None, bold_table=False):\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    for m in metrics:\n",
    "        df[m] = df[m].apply(lambda x: str(round(x*100, 1)))\n",
    "\n",
    "    final_df = df[[\"rmodel\"] + metrics]\n",
    "    final_df[\"rmodel\"] = final_df[\"rmodel\"].apply(lambda x: rename_model(x))\n",
    "    final_df = final_df.rename(columns={\"rmodel\": \"Model\"})\n",
    "\n",
    "    if model_order:\n",
    "        final_df = final_df.loc[[m for m in model_order if m in final_df.index]]\n",
    "    \n",
    "    if not bold_table:\n",
    "        final_df[\"Model\"] = final_df[\"Model\"].apply(lambda x: bold_str(x))\n",
    "        final_df = final_df.rename(bold_str, axis=\"columns\")\n",
    "\n",
    "    if output_dir:\n",
    "        final_df.to_latex(os.path.join(output_dir, \"retrieval-augmented-classification.tex\"), index=False, caption=\"In-context learning results.\")\n",
    "\n",
    "    return final_df.to_latex(index=False, caption=\"Retrieval-augmented classification results.\").replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()\", \"test_lang:en:BinaryRecall()\", \"test_lang:en:BinaryF1Score()\", \"test_lang:de:BinaryPrecision()\", \"test_lang:de:BinaryRecall()\", \"test_lang:de:BinaryF1Score()\", \"test_BinaryF1Score()\"]\n",
    "print(make_table_rac(full_df[full_df[\"result_type\"].isin([\"rac\"])], metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_best_run(df, metrics, stds):\n",
    "    best_rows = defaultdict(list)\n",
    "    for metric in metrics:\n",
    "        best_row = get_best_row(df, metric)\n",
    "        best_cmodel = best_row[\"cmodel\"]\n",
    "        weight = best_row[\"weight\"]\n",
    "\n",
    "        for cname, cgroup in df.groupby(by=\"cmodel\"):\n",
    "            if cname == best_cmodel:\n",
    "                for rname, rgroup in cgroup.groupby(by=\"rmodel\"):\n",
    "                    _rgroup = rgroup[rgroup[\"weight\"] == weight].reset_index(drop=True)\n",
    "                    best_rmodel_row = get_best_row(_rgroup, metric)\n",
    "                    best_rows[metric].append(best_rmodel_row)\n",
    "\n",
    "                    _rgroup = rgroup[rgroup[\"weight\"] == '1.0'].reset_index(drop=True)\n",
    "                    best_rmodel_row = get_best_row(_rgroup, metric)\n",
    "                    best_rows[metric].append(best_rmodel_row)\n",
    "\n",
    "        no_retrieval_row = df[(df[\"cmodel\"] == best_cmodel) & (df[\"weight\"] == '0.0')].drop_duplicates(\"cmodel\").iloc[0]\n",
    "        best_rows[metric].append(no_retrieval_row.to_dict())\n",
    "        \n",
    "    best_df = pd.DataFrame(best_rows[metrics[-1]])[[\"cmodel\", \"rmodel\", \"weight\"]+metrics+stds]  # choose best metric to show\n",
    "    return best_df\n",
    "\n",
    "def make_table_rac(df, max_val_metric, metrics, stds, model_order):\n",
    "    best_cmodel = get_best_row(df[df[\"weight\"] == \"0.0\"], max_val_metric)\n",
    "    cmodel = best_cmodel[\"cmodel\"]\n",
    "\n",
    "    best_rmodel = get_best_row(df[df[\"cmodel\"] == cmodel], max_val_metric)\n",
    "    weight = best_rmodel[\"weight\"]\n",
    "    df = df[(df[\"weight\"] == weight) & (df[\"cmodel\"] == cmodel)]\n",
    "\n",
    "    df = pd.concat([pd.DataFrame([best_cmodel]), df])\n",
    "\n",
    "    new_rows = []\n",
    "    metric_cols = []\n",
    "    for i in range(df.shape[0]):\n",
    "        row = deepcopy(df.iloc[i])\n",
    "        for m,s in zip(metrics, stds):\n",
    "            metric = m.split(\"Binary\")[-1].split(\"()\")[0]\n",
    "            if \":\" in m:\n",
    "                lang = m.split(\":\")[1].upper()\n",
    "                name = metric+\" \"+lang\n",
    "            else:\n",
    "                name = metric\n",
    "            if name not in metric_cols:\n",
    "                metric_cols.append(name)\n",
    "            rounded_mean = str(round(row[m]*100, 1))\n",
    "            rounded_std = str(round(row[s]*100, 0)).replace('0.', '.').replace('.00', '')\n",
    "            row[name] = rounded_mean+\"$_{\\pm\"+rounded_std+\"}$\"\n",
    "        new_rows.append(row)\n",
    "    \n",
    "    cols_to_keep = [\"rmodel\", \"weight\"] + list(metric_cols)\n",
    "\n",
    "    final_df = pd.DataFrame(new_rows)[cols_to_keep]\n",
    "    final_df[\"rmodel\"] = final_df[\"rmodel\"].apply(lambda x: rename_model(x))\n",
    "    final_df[\"weight\"] = final_df[\"weight\"].apply(lambda x: str(x))\n",
    "    final_df = final_df.rename(columns={\"rmodel\": \"Model\"})\n",
    "    final_df.index = final_df[\"Model\"]\n",
    "    final_df = final_df.loc[[m for m in model_order if m in final_df.index]]\n",
    "    final_df[\"Model\"] = final_df[\"Model\"].apply(lambda x: bold_str(x))\n",
    "    final_df = final_df.rename(bold_str, axis=\"columns\")\n",
    "\n",
    "    if output_dir:\n",
    "        final_df.to_latex(os.path.join(output_dir, \"best_run.tex\"), index=False, caption=f\"Retrieval-augmented classification using {rename_model(cmodel)} as the base classifier.\").replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")\n",
    "    return final_df.to_latex(index=False, caption=f\"Retrieval-augmented classification using {rename_model(cmodel)} as the base classifier.\").replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val_metric = \"val_eval_BinaryF1Score()_mean\"\n",
    "test_metrics = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "stds = [m.replace(\"mean\", \"std\") for m in metrics]\n",
    "model_order = [\"BERT\", \"mBERT\", \"mBERT-ca\", \"mBERT-unca\", \"DistilBERT\", \"RoBERTa\", \"SciBERT\", \"MiniLM\", \"mMiniLM\", \"mMPNet\", \"SPECTER\", \"Sentence-T5\", \"SsciBERT\", \"E5\", \"mE5\"]\n",
    "print(make_table_rac(full_df, max_val_metric, test_metrics, stds, model_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_results_df(results_dir):\n",
    "    run_dirs = [os.path.join(results_dir, r) for r in os.listdir(results_dir) if \"_\" != r[0]]\n",
    "\n",
    "    run_results = {}\n",
    "\n",
    "    for run_dir in run_dirs:\n",
    "        runs = [os.path.join(run_dir, r) for r in os.listdir(run_dir) if os.path.isdir(os.path.join(run_dir, r))]\n",
    "\n",
    "        results = []\n",
    "        for run in runs:\n",
    "            results_file = os.path.join(run, \"results.json\")\n",
    "            if os.path.isfile(results_file):\n",
    "                data = load_json(results_file)\n",
    "\n",
    "                res = {}\n",
    "                metrics = data[\"config\"][\"metrics\"].split(\",\")\n",
    "                for m in metrics:\n",
    "                    res[m] = round(data[\"results\"][m]*100, 2)\n",
    "                \n",
    "                for attribute, attribute_results in data[\"fg_results\"].items():\n",
    "                    for category, category_results in attribute_results.items():\n",
    "                        for m in metrics:\n",
    "                            name = f\"{attribute}_{category}_{m}\"\n",
    "                            res[name] = round(category_results[m]*100,2)\n",
    "\n",
    "                res = res | {\n",
    "                    \"model\": data[\"config\"][\"model_name\"],\n",
    "                    \"reranker\": data[\"config\"][\"reranker_name\"],\n",
    "                    \"dense\": data[\"config\"][\"do_dense\"],\n",
    "                    \"sparse\": data[\"config\"][\"do_sparse\"],\n",
    "                    \"join\": data[\"config\"][\"do_join\"],\n",
    "                    \"rerank\": data[\"config\"][\"do_rerank\"],\n",
    "                    \"subset_datastore\": data[\"config\"][\"do_subset_datastore\"],\n",
    "                    \"filter\": data[\"config\"][\"do_filter\"],\n",
    "                    \"filters\": \";\".join(data[\"config\"][\"filters\"]),\n",
    "                    \"metadata_values\": data[\"config\"][\"metadata_values\"],\n",
    "                    \"seed\": data[\"config\"][\"seed\"],\n",
    "                    \"docstore\": data[\"config\"][\"doc_store_type\"],\n",
    "                }\n",
    "                results.append(res)\n",
    "        output_path = os.path.join(run_dir, \"results.tsv\")\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(output_path, index=False, sep=\"\\t\")\n",
    "\n",
    "        run_results[run_dir] = results\n",
    "\n",
    "    return run_results\n",
    "\n",
    "\n",
    "def get_nested_results_df(results_dir):\n",
    "    run_dirs = [os.path.join(results_dir, r) for r in os.listdir(results_dir) if \"_\" != r[0]]\n",
    "\n",
    "    run_results = {}\n",
    "\n",
    "    for run_dir in run_dirs:\n",
    "        days = [os.path.join(run_dir, r) for r in os.listdir(run_dir) if os.path.isdir(os.path.join(run_dir, r))]  # days\n",
    "\n",
    "        results = []\n",
    "        for day in days:\n",
    "            samples = [os.path.join(day, d) for d in os.listdir(day) if os.path.isdir(os.path.join(day, d))]  # samples\n",
    "\n",
    "            for sample in samples:\n",
    "                seeds = [os.path.join(sample, s) for s in os.listdir(sample) if os.path.isdir(os.path.join(sample, s))]  # seeds\n",
    "\n",
    "                for seed in seeds:\n",
    "                    runs = [os.path.join(seed, s) for s in os.listdir(seed) if os.path.isdir(os.path.join(seed, s))]  # runs\n",
    "\n",
    "                    for run in runs:\n",
    "                        results_file = os.path.join(run, \"results.json\")\n",
    "                        if os.path.isfile(results_file):\n",
    "                            data = load_json(results_file)\n",
    "\n",
    "                            res = {}\n",
    "                            metrics = data[\"config\"][\"metrics\"].split(\",\")\n",
    "                            for m in metrics:\n",
    "                                res[m] = round(data[\"results\"][m]*100, 2)\n",
    "                            \n",
    "                            for attribute, attribute_results in data[\"fg_results\"].items():\n",
    "                                for category, category_results in attribute_results.items():\n",
    "                                    for m in metrics:\n",
    "                                        name = f\"{attribute}_{category}_{m}\"\n",
    "                                        res[name] = round(category_results[m]*100,2)\n",
    "\n",
    "                            res = res | {\n",
    "                                \"day\": os.path.basename(day),\n",
    "                                \"sample\": os.path.basename(sample),\n",
    "                                \"seed\": os.path.basename(seed),\n",
    "                                \"model\": data[\"config\"][\"model_name\"].split(\"/\")[-1].split(\":\")[-1].split(\"_epochs=\")[0],\n",
    "                                \"reranker\": data[\"config\"][\"reranker_name\"],\n",
    "                                \"dense\": data[\"config\"][\"do_dense\"],\n",
    "                                \"sparse\": data[\"config\"][\"do_sparse\"],\n",
    "                                \"join\": data[\"config\"][\"do_join\"],\n",
    "                                \"rerank\": data[\"config\"][\"do_rerank\"],\n",
    "                                \"subset_datastore\": data[\"config\"][\"do_subset_datastore\"],\n",
    "                                \"filter\": data[\"config\"][\"do_filter\"],\n",
    "                                \"filters\": \";\".join(data[\"config\"][\"filters\"]),\n",
    "                                \"metadata_values\": data[\"config\"][\"metadata_values\"],\n",
    "                                \"docstore\": data[\"config\"][\"doc_store_type\"],\n",
    "                            }\n",
    "                            results.append(res)\n",
    "\n",
    "        output_path = os.path.join(run_dir, \"results.tsv\")\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(output_path, index=False, sep=\"\\t\")\n",
    "\n",
    "        run_results[run_dir] = results\n",
    "    \n",
    "    return run_results\n",
    "\n",
    "def get_nested_results_df_v2(results_dir):\n",
    "    days = [os.path.join(results_dir, r) for r in os.listdir(results_dir) if os.path.isdir(os.path.join(results_dir, r))]  # days\n",
    "\n",
    "    results = []\n",
    "    for day in days:\n",
    "        samples = [os.path.join(day, d) for d in os.listdir(day) if os.path.isdir(os.path.join(day, d))]  # samples\n",
    "\n",
    "        for sample in samples:\n",
    "            seeds = [os.path.join(sample, s) for s in os.listdir(sample) if os.path.isdir(os.path.join(sample, s))]  # seeds\n",
    "\n",
    "            for seed in seeds:\n",
    "                runs = [os.path.join(seed, s) for s in os.listdir(seed) if os.path.isdir(os.path.join(seed, s))]  # runs\n",
    "\n",
    "                for run in runs:\n",
    "                    results_file = os.path.join(run, \"results.json\")\n",
    "                    if os.path.isfile(results_file):\n",
    "                        data = load_json(results_file)\n",
    "\n",
    "                        res = {}\n",
    "                        metrics = data[\"config\"][\"metrics\"].split(\",\")\n",
    "                        for m in metrics:\n",
    "                            res[m] = round(data[\"results\"][m]*100, 2)\n",
    "                        \n",
    "                        for attribute, attribute_results in data[\"fg_results\"].items():\n",
    "                            for category, category_results in attribute_results.items():\n",
    "                                for m in metrics:\n",
    "                                    name = f\"{attribute}_{category}_{m}\"\n",
    "                                    res[name] = round(category_results[m]*100,2)\n",
    "\n",
    "                        res = res | {\n",
    "                            \"day\": os.path.basename(day),\n",
    "                            \"sample\": os.path.basename(sample),\n",
    "                            \"seed\": os.path.basename(seed),\n",
    "                            \"model\": data[\"config\"][\"model_name\"].split(\"/\")[-1].split(\":\")[-1].split(\"_epochs=\")[0],\n",
    "                            \"reranker\": data[\"config\"][\"reranker_name\"],\n",
    "                            \"dense\": data[\"config\"][\"do_dense\"],\n",
    "                            \"sparse\": data[\"config\"][\"do_sparse\"],\n",
    "                            \"join\": data[\"config\"][\"do_join\"],\n",
    "                            \"rerank\": data[\"config\"][\"do_rerank\"],\n",
    "                            \"subset_datastore\": data[\"config\"][\"do_subset_datastore\"],\n",
    "                            \"filter\": data[\"config\"][\"do_filter\"],\n",
    "                            \"filters\": \";\".join(data[\"config\"][\"filters\"]),\n",
    "                            \"metadata_values\": data[\"config\"][\"metadata_values\"],\n",
    "                            \"docstore\": data[\"config\"][\"doc_store_type\"],\n",
    "                        }\n",
    "                        results.append(res)\n",
    "\n",
    "    output_path = os.path.join(results_dir, \"results.tsv\")\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_path, index=False, sep=\"\\t\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "results_dir = \"../results/runs_task2_results\"\n",
    "pt_results = get_results_df(results_dir)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "results_dir = \"../results/task2_results_simlearn-17-01-23/filtered\"\n",
    "ft_results = get_nested_results_df(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "metrics = [\"lang_en_recall@10\", \"lang_en_map@10\", \"lang_en_ndcg@10\", \"lang_de_recall@10\", \"lang_de_map@10\", \"lang_de_ndcg@10\", \"recall@10\", \"map@10\", \"ndcg@10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sparse_filter_df = pd.DataFrame(pt_results['../results/runs_task2_results/run_bm25_exp_filter_subset'])[metrics]\n",
    "sparse_filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sparse_nofilter_df = pd.DataFrame(pt_results['../results/runs_task2_results/run_bm25_exp_no-subset_no-filter'])[metrics]\n",
    "sparse_nofilter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pre-trained sentence embeddings baselines\n",
    "\n",
    "model_order = [\"XLM-R$_{\\text{base}}$\", \"XLM-R$_{\\text{large}}$\", \"SoSci-XLM-R$_{\\text{base}}$\", \"Para\", \"Cross$_{en-de}$\", \"mE5$_{small}$\", \"mE5$_{base}$\", \"mE5$_{large}$\"]\n",
    "\n",
    "dense_filter_df = pd.DataFrame(pt_results['../results/runs_task2_results/run_multilingual_model_choice_exp_baseline'])[[\"model\"]+metrics]\n",
    "dense_filter_df[\"model\"] = dense_filter_df[\"model\"].apply(lambda x: rename_model(x))\n",
    "dense_filter_df.index = dense_filter_df[\"model\"]\n",
    "print(dense_filter_df.loc[model_order].to_latex(index=False).replace(\"0000\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_sosse_df(sosse_path, baseline_path):\n",
    "    filtered = get_nested_results_df_v2(sosse_path)\n",
    "    filtered_df = pd.DataFrame(filtered)\n",
    "\n",
    "    _dense_df = pd.DataFrame(pt_results[baseline_path])\n",
    "\n",
    "    _dense_df[\"sample\"] = \"sample=0\"\n",
    "    _dense_df[\"model\"] = _dense_df[\"model\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    _dense_df = _dense_df[_dense_df[\"model\"].isin(filtered_df[\"model\"].unique())]\n",
    "\n",
    "    filtered_df = pd.concat([filtered_df, _dense_df])\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def make_sosse_dataset_size_plot(df, metrics, metrics_clean=None, output_dir=None, n_columns=3, fontsize=18, figsize=(30,10), y_offset=1, add_title=True):\n",
    "    n_subplots = len(metrics)\n",
    "    n_rows = int(n_subplots / n_columns) if n_subplots % n_columns == 0 else int((n_subplots / n_columns) + 1)\n",
    "    y_min = math.floor(min(df[metrics].min().tolist())-y_offset)\n",
    "    y_max = math.ceil(max(df[metrics].max().tolist())+y_offset)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_columns, figsize=figsize)\n",
    "\n",
    "    c = 0\n",
    "    r = 0\n",
    "    for i, metric in enumerate(metrics):\n",
    "        # print(\"Metric:\", metric)\n",
    "        if n_rows > 1:\n",
    "            ax = axes[r, c]\n",
    "        else:\n",
    "            ax = axes[c]\n",
    "\n",
    "        samples = [\"sample=200\", \"sample=2000\", \"sample=20000\", \"sample=200000\", \"sample=400000\"]\n",
    "        x_values = [int(x.split(\"sample=\")[-1]) for x in samples]\n",
    "\n",
    "        for model,model_group in df.groupby(by=\"model\"):\n",
    "            # print(model)\n",
    "            y_means = []\n",
    "            y_stds = []\n",
    "\n",
    "            for sample in samples:\n",
    "                y_mean = model_group[model_group[\"sample\"] == sample][metric].mean()\n",
    "                y_std = model_group[model_group[\"sample\"] == sample][metric].std()\n",
    "\n",
    "                if y_mean and y_std:\n",
    "                    y_means.append(y_mean)\n",
    "                    y_stds.append(y_std)\n",
    "                else:\n",
    "                    y_means.append(0)\n",
    "                    y_stds.append(0)\n",
    "\n",
    "            y_means = np.asarray(y_means)\n",
    "            y_stds = np.asarray(y_stds)\n",
    "            \n",
    "            line, = ax.plot(x_values, y_means, label=rename_model(model))\n",
    "            line_style = line.get_linestyle()\n",
    "            ax.fill_between(x_values, y_means-y_stds, y_means+y_stds, alpha=0.2)\n",
    "\n",
    "            y_baseline = [model_group[model_group[\"sample\"] == \"sample=0\"][metric].mean()] * len(x_values)\n",
    "            ax.plot(x_values, y_baseline, label=rename_model(model)+\"$^{\\clubsuit}$\", color='gray', linestyle=line_style)\n",
    "            \n",
    "        ax.tick_params(axis='x', labelsize=fontsize)\n",
    "        ax.tick_params(axis='y', labelsize=fontsize)\n",
    "\n",
    "        if add_title:\n",
    "            metric_title = metric\n",
    "            if metrics_clean:\n",
    "                metric_title = rename_metric(metrics_clean[i])\n",
    "            ax.set_title(metric_title, fontsize=fontsize)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        axes[0].legend(handles, labels, borderaxespad=0.1, loc=\"best\", fancybox=True, framealpha=0.5, fontsize=fontsize).set_title(\"Models\",prop={'size':fontsize})\n",
    "\n",
    "        if r == n_rows - 1:\n",
    "            c += 1\n",
    "            r = 0\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    fig.supxlabel(\"Dataset size\", fontsize=fontsize)\n",
    "    fig.supylabel(rename_metric(metric).split(\"_\")[-1], fontsize=fontsize, x=0.005)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, os.path.basename(output_dir)+\".pdf\")\n",
    "        plt.savefig(path, bbox_inches=\"tight\")\n",
    "        print(f\"Save to path: {path}\")\n",
    "        \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sosse_path = \"../results/task2_results_sosci-simlearn/filtered/run_multilingual_model_choice_exp\"\n",
    "baseline_path = \"../results/runs_task2_results/run_multilingual_model_choice_exp_baseline\"\n",
    "\n",
    "results_dir = \"../results/runs_task2_results\"\n",
    "pt_results = get_results_df(results_dir)\n",
    "\n",
    "filtered_df = get_sosse_df(sosse_path, baseline_path)\n",
    "metrics = [\"lang_en_map@10\", \"lang_de_map@10\"]\n",
    "metrics_clean = [\"English\", \"German\"]\n",
    "make_sosse_dataset_size_plot(filtered_df, metrics, metrics_clean, add_title=True, n_columns=2, figsize=(20,10), output_dir=os.path.join(output_dir, \"SoSSE_filtered\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sosse_path = \"../results/task2_results_sosci-simlearn/filtered_gen/run_multilingual_model_choice_exp\"\n",
    "baseline_path = \"../results/runs_task2_results/run_multilingual_model_choice_exp_baseline\"\n",
    "\n",
    "filtered_df = get_sosse_df(sosse_path, baseline_path)\n",
    "metrics = [\"lang_en_map@10\", \"lang_de_map@10\"]\n",
    "metrics_clean = [\"English\", \"German\"]\n",
    "make_sosse_dataset_size_plot(filtered_df, metrics, metrics_clean, add_title=True, n_columns=2, figsize=(20,10), output_dir=os.path.join(output_dir, \"SoSSE_filtered_gen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pick best model on validation data\n",
    "sosse_path = \"../results/task2_results_sosci-simlearn/filtered/run_sim_data_size_choice_exp/04-04-2024\"\n",
    "baseline_path = \"../results/runs_task2_results/run_multilingual_model_choice_exp_baseline\"\n",
    "\n",
    "results_dir = \"../results/runs_task2_results\"\n",
    "pt_results = get_results_df(results_dir)\n",
    "\n",
    "filtered_df = get_sosse_df(sosse_path, baseline_path)\n",
    "filtered_df[[\"recall@10\", \"map@10\", \"ndcg@10\", \"model\", \"seed\", \"sample\"]].sort_values(by=\"map@10\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pick best model on test data\n",
    "sosse_path = \"../results/task2_results_sosci-simlearn/filtered_gen/run_multilingual_model_choice_exp\"\n",
    "baseline_path = \"../results/runs_task2_results/run_multilingual_model_choice_exp_baseline\"\n",
    "\n",
    "results_dir = \"../results/runs_task2_results\"\n",
    "pt_results = get_results_df(results_dir)\n",
    "\n",
    "filtered_df = get_sosse_df(sosse_path, baseline_path)\n",
    "filtered_df = filtered_df[filtered_df[\"sample\"].isin([\"sample=0\", \"sample=200000\"])]\n",
    "filtered_df[[\"recall@10\", \"map@10\", \"ndcg@10\", \"model\", \"seed\", \"sample\"]].sort_values(by=\"map@10\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "filtered_df[[\"lang_en_map@10\", \"lang_de_map@10\", \"map@10\", \"model\", \"seed\", \"sample\"]].sort_values(by=\"map@10\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "res = [\n",
    "    {\"model\": \"Para\", \"EN\": 57.2, \"DE\": 58.6, \"Total\": 57.6, \"sample\": 0},\n",
    "    {\"model\": \"Para\", \"EN\": 58.8, \"DE\": 61.9, \"Total\": 59.7, \"sample\": 200000},\n",
    "    {\"model\": \"Cross\", \"EN\": 49.1, \"DE\": 51.7, \"Total\": 49.8, \"sample\": 0},\n",
    "    {\"model\": \"Cross\", \"EN\": 50.7, \"DE\": 58.8, \"Total\": 53.1, \"sample\": 200000},\n",
    "    {\"model\": \"mE5$_{\\text{base}}$\", \"EN\": 57.9, \"DE\": 65.6, \"Total\": 60.2, \"sample\": 0},\n",
    "    {\"model\": \"mE5$_{\\text{base}}$\", \"EN\": 63.4, \"DE\": 68.2, \"Total\": 64.9, \"sample\": 200000},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame(res).to_latex(index=False).replace(\"00000\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def make_table_sosse(df, metrics, keep_weights=None, keep_algorithms=None, p_val=0.05, output_dir=None, bold_table=False):\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    rows = []\n",
    "    # group by values\n",
    "    for model, model_group in df.groupby(\"model\"):\n",
    "        for sample, sample_group in model_group.groupby(\"sample\"):\n",
    "            _row = {}\n",
    "            _row[\"model\"] = rename_model(model)\n",
    "            _row[\"sample\"] = sample\n",
    "            for metric in metrics:\n",
    "                avg_score = round(sample_group[metric].mean(), 1)\n",
    "                std_score = sample_group[metric].std()\n",
    "\n",
    "                score_str = f\"{avg_score}\"\n",
    "                if str(std_score) != \"nan\":\n",
    "                    score_str += \"$_{\" + f\"\\pm{round(std_score,1)}\" + \"}$\"\n",
    "                _row[f\"{metric}_str\"] = score_str\n",
    "            rows.append(_row)\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame(make_table_sosse(filtered_df, [\"lang_en_map@10\", \"lang_de_map@10\"])).to_latex(index=False).replace(\"sample=0\", \"-\").replace(\"sample=\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_path = \"../results/data/vadis-prolific-3_project_2023-12-09_1251_12:53:08_htest.tsv\"\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\")\n",
    "test_df = test_df.dropna(subset=[\"is_variable\"])\n",
    "test_df[\"is_variable\"] = test_df[\"is_variable\"].apply(lambda x: int(x))\n",
    "test_df[\"label\"] = test_df[\"is_variable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "classifier_preds_path = \"../results/runs_sv4_journal_paper/finetune_best-2024-03-26_15-24/FacebookAI--xlm-roberta-large-finetuned_do-retrieval=False_20240326-152437/seed=42/fold=0/test_preds.tsv\"\n",
    "disambiguator_bm25_preds_path = \"../results/runs_task2_results/run_bm25_exp_filter_subset/20240410-183115/run.json\"\n",
    "disambiguator_st_preds_path = \"../results/task2_results_sosci-simlearn/filtered_gen/run_best_ed_model/04-10-2024/20240410-225722/run.json\"\n",
    "\n",
    "qrels_path = \"../results/runs_task2_results/run_bm25_exp_filter_subset/20240410-183115/qrels.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "classifier_preds_df = pd.read_csv(classifier_preds_path, sep=\"\\t\")\n",
    "classifier_preds_df.index = classifier_preds_df[\"uuid\"]\n",
    "classifier_preds_df[\"pred_score\"] = classifier_preds_df[\"pred_scores\"].apply(lambda x: float(x.split(\";\")[-1]))\n",
    "\n",
    "pos_classified_uuids = classifier_preds_df[classifier_preds_df[\"pred\"].isin([1, \"1\"])][\"uuid\"].tolist()\n",
    "\n",
    "disambiguator_bm25_preds = load_json(disambiguator_bm25_preds_path)\n",
    "disambiguator_pt_preds = load_json(disambiguator_st_preds_path)\n",
    "qrels_dict = load_json(qrels_path)\n",
    "\n",
    "full_qrels_dict = {uid: qrels_dict.get(uid, {}) for uid in test_df[\"uuid\"].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "disambiguator_bm25_preds_filtered = {k:(v if k in pos_classified_uuids else {}) for k,v in disambiguator_bm25_preds.items()}\n",
    "disambiguator_pt_preds_filtered = {k:(v if k in pos_classified_uuids else {}) for k,v in disambiguator_pt_preds.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from ranx import Qrels, Run, evaluate\n",
    "\n",
    "def get_scores(qrels_dict, run_dict, valid_uuids=None, metrics=[\"f1@10\", \"precision@10\", \"recall@10\", \"f1@10\", \"map@10\", \"ndcg@10\"]):\n",
    "    if valid_uuids:\n",
    "        qrels_dict = {k:v for k,v in qrels_dict.items() if k in valid_uuids}\n",
    "        run_dict = {k:v for k,v in run_dict.items() if k in valid_uuids}\n",
    "\n",
    "    qrels = Qrels(qrels_dict)\n",
    "    run = Run(run_dict)\n",
    "\n",
    "    results = evaluate(qrels, run, metrics)\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_scores(filtered_preds, perfect_preds, qrels_dict, perfect_qrels_dict, test_df):\n",
    "    en_uuids = test_df[test_df[\"lang\"] == \"en\"][\"uuid\"].tolist()\n",
    "    de_uuids = test_df[test_df[\"lang\"] == \"de\"][\"uuid\"].tolist()\n",
    "\n",
    "    en_scores = get_scores(qrels_dict, filtered_preds, en_uuids)\n",
    "    de_scores = get_scores(qrels_dict, filtered_preds, de_uuids)\n",
    "    full_scores = get_scores(qrels_dict, filtered_preds)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # perfect entity mention detection\n",
    "    perf_en_scores = get_scores(perfect_qrels_dict, perfect_preds, en_uuids)\n",
    "    perf_de_scores = get_scores(perfect_qrels_dict, perfect_preds, de_uuids)\n",
    "    perf_full_scores = get_scores(perfect_qrels_dict, perfect_preds)\n",
    "\n",
    "    return {\"fil_en\": en_scores, \"fil_de\": de_scores, \"fil_full\": full_scores, \"perf_en\": perf_en_scores, \"perf_de\": perf_de_scores, \"perf_full\": perf_full_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare scores for BM25\n",
    "scores = compare_scores(disambiguator_bm25_preds_filtered, disambiguator_bm25_preds, qrels_dict, qrels_dict, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare scores for best sentence transformer\n",
    "scores = compare_scores(disambiguator_pt_preds_filtered, disambiguator_pt_preds, qrels_dict, qrels_dict, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "full_disambiguator_bm25_preds_path = \"../results/runs_task2_results/run_bm25_exp_filter_subset/20240411-080029/run.json\"\n",
    "full_qrels_path = \"../results/runs_task2_results/run_bm25_exp_filter_subset/20240411-080029/run.json\"\n",
    "\n",
    "full_disambiguator_bm25_preds = load_json(full_disambiguator_bm25_preds_path)\n",
    "full_qrels = load_json(full_qrels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pred_pos_uuids = []\n",
    "\n",
    "for k,v in full_disambiguator_bm25_preds.items():\n",
    "    for k1,v1 in v.items():\n",
    "        if v1 >= 0.99:\n",
    "            pred_pos_uuids.append(k)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "retrieval_pt_preds = {k:v for k,v in full_disambiguator_bm25_preds.items() if k in pred_pos_uuids}\n",
    "retrieval_qrels_dict = {k:v for k,v in full_qrels.items() if k in pred_pos_uuids}\n",
    "\n",
    "scores = compare_scores(retrieval_pt_preds, full_disambiguator_bm25_preds, retrieval_qrels_dict, full_qrels, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "results_dir = \"../results/runs_task2_results\"\n",
    "pt_results = get_results_df(results_dir)\n",
    "\n",
    "results_dir = \"../results/task2_results_simlearn-17-01-23/filtered\"\n",
    "ft_results = get_nested_results_df(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "metrics = [\"lang_en_recall@10\", \"lang_en_map@10\", \"lang_en_ndcg@10\", \"lang_de_recall@10\", \"lang_de_map@10\", \"lang_de_ndcg@10\", \"recall@10\", \"map@10\", \"ndcg@10\"]\n",
    "sparse_nofilter_df = pd.DataFrame(pt_results['../results/runs_task2_results/run_bm25_exp_no-subset_no-filter'])[metrics]\n",
    "sparse_nofilter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def make_table_diagnostic(df, metrics, model_order, keep_weights=None, keep_algorithms=[\"Unk\"], p_val=0.05, output_dir=None):\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df[\"_model\"] = df[\"cmodel\"].apply(lambda x: rename_model(x))\n",
    "    df = df[df[\"_model\"].isin(model_order)]\n",
    "    df = df.drop(columns=[\"_model\"])\n",
    "        \n",
    "    df, stds, raws = pre_processing(df, metrics, model_cols=[\"cmodel\"], filter_criteria=[(\"algorithm\", keep_algorithms), (\"weight\", keep_weights), (\"rmodel\", None)])\n",
    "\n",
    "    new_rows, cols_to_keep = format_scores(df, metrics, stds, [\"cmodel\", \"algorithm\"])\n",
    "    final_df = pd.DataFrame(new_rows)[cols_to_keep].reset_index(drop=True)\n",
    "    raw_df = pd.DataFrame(new_rows)[cols_to_keep+raws].reset_index(drop=True)\n",
    "\n",
    "    final_df = format_table(final_df, model_order)\n",
    "\n",
    "    if output_dir:\n",
    "        final_df.to_latex(os.path.join(output_dir, \"finetune.tex\"), index=False, caption=\"Fine-tuned transformers.\")\n",
    "\n",
    "    return final_df.to_latex(index=False, caption=\"Fine-tuned transformers.\").replace(\"Precision\", \"$P$\").replace(\"Recall\", \"$R$\").replace(\"F1Score\", \"$F_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# performance across types and subtypes\n",
    "metrics = [\"test_type_short:E:BinaryF1Score()_mean\", \"test_type_short:I:BinaryF1Score()_mean\", \"test_subtype_short:Q:BinaryF1Score()_mean\", \"test_subtype_short:P:BinaryF1Score()_mean\"]\n",
    "\n",
    "model_order = [\"XLM-R$_{\\text{base}}$\", \"XLM-R$_{\\text{large}}$\", \"SoSci-XLM-R\"]\n",
    "print(make_table_diagnostic(full_df[full_df[\"result_type\"].isin([\"finetune\"])], metrics, model_order, output_dir=os.path.join(output_dir, \"md_diagnostics\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "metrics = [\"type_short_E_recall@10\", \"type_short_I_recall@10\", \"subtype_short_Q_recall@10\", \"subtype_short_P_recall@10\"]\n",
    "\n",
    "model_order = [\"mE5$_{small}$\", \"mE5$_{base}$\", \"mE5$_{large}$\"]\n",
    "\n",
    "dense_filter_df = pd.DataFrame(pt_results['../results/runs_task2_results/run_multilingual_model_choice_exp_baseline'])[[\"model\"]+metrics]\n",
    "dense_filter_df[\"model\"] = dense_filter_df[\"model\"].apply(lambda x: rename_model(x))\n",
    "dense_filter_df.index = dense_filter_df[\"model\"]\n",
    "print(dense_filter_df.loc[model_order].to_latex(index=False).replace(\"0000\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sparse_filter_df = pd.DataFrame(pt_results['../results/runs_task2_results/run_bm25_exp_filter_subset'])[[\"model\"]+metrics]\n",
    "sparse_filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sosse_path = \"../results/task2_results_sosci-simlearn/filtered/run_sim_data_size_choice_exp\"\n",
    "baseline_path = \"../results/runs_task2_results/run_multilingual_model_choice_exp_baseline\"\n",
    "\n",
    "results_dir = \"../results/runs_task2_results\"\n",
    "pt_results = get_results_df(results_dir)\n",
    "\n",
    "filtered_df = get_sosse_df(sosse_path, baseline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = filtered_df[(filtered_df[\"model\"].isin([\"multilingual-e5-base\"])) & (filtered_df[\"sample\"].isin([\"sample=200000\"]))][[\"model\", \"sample\", \"seed\"]+metrics]\n",
    "_df[\"model\"] = _df[\"model\"].apply(lambda x: rename_model(x))\n",
    "_df[metrics].mean(), _df[metrics].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "single_item_uuids = {}\n",
    "multi_item_uuids = {}\n",
    "\n",
    "test_df[\"label\"] = test_df[\"label\"].fillna(0)\n",
    "\n",
    "for i in range(test_df.shape[0]):\n",
    "    row = test_df.iloc[i]\n",
    "\n",
    "    label = int(row.get(\"label\", 0))\n",
    "    uuid = row.get(\"uuid\")\n",
    "\n",
    "    if label == 1:\n",
    "        variables = row.get(\"variable\", \"\").split(\";\")\n",
    "        variables = [v for vs in variables for v in vs.split(\",\") if v]\n",
    "\n",
    "        if len(variables) > 1:\n",
    "            multi_item_uuids[uuid] = variables\n",
    "        else:\n",
    "            single_item_uuids[uuid] = variables\n",
    "\n",
    "single_plus_non_multi_uuids = list(single_item_uuids.keys()) + [uid for uid in test_df[\"uuid\"].tolist() if uid not in multi_item_uuids]\n",
    "multi_plus_non_multi_uuids = list(multi_item_uuids.keys()) + [uid for uid in test_df[\"uuid\"].tolist() if uid not in single_item_uuids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from metrics import compute_metrics\n",
    "\n",
    "def compute_results_subset(files, gold_file, uuids):\n",
    "    true_df = pd.read_csv(gold_file, sep=\"\\t\")\n",
    "    true_df.index = true_df[\"uuid\"]\n",
    "    true_df[\"is_variable\"] = true_df[\"is_variable\"].fillna(0)\n",
    "    true = true_df.loc[uuids][\"is_variable\"].astype(int).tolist()\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for f in files:\n",
    "        preds_df = pd.read_csv(f, sep=\"\\t\")\n",
    "        preds_df.index = preds_df[\"uuid\"]\n",
    "        preds_df[\"pred_scores\"] = preds_df[\"pred_scores\"].apply(lambda x: [float(x.split(\";\")[0]), float(x.split(\";\")[1])])\n",
    "\n",
    "        pred_scores = preds_df.loc[uuids][\"pred_scores\"].tolist()\n",
    "\n",
    "        scores = compute_metrics([pred_scores, true])\n",
    "        for k,v in scores.items():\n",
    "            results[k].append(v)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "best_ed_model_path = \"../results/runs_sv4_journal_paper_results_14-03-2024/finetune-2024-03-11_11-26/FacebookAI--xlm-roberta-large-finetuned_do-retrieval=False_20240311-112506\"\n",
    "seed_dirs = [os.path.join(best_ed_model_path, s, \"fold=0\", \"test_preds.tsv\") for s in os.listdir(best_ed_model_path) if os.path.isdir(os.path.join(best_ed_model_path, s))]\n",
    "\n",
    "sresults = compute_results_subset(seed_dirs, \"../results/data/vadis-prolific-3_project_2023-12-09_1251_12:53:08_htest.tsv\", single_plus_non_multi_uuids)\n",
    "mresults = compute_results_subset(seed_dirs, \"../results/data/vadis-prolific-3_project_2023-12-09_1251_12:53:08_htest.tsv\", multi_plus_non_multi_uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_mean_std(scores):\n",
    "    mean = round(np.mean(scores)*100, 1)\n",
    "    std = round(np.std(scores)*100, 0)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "metrics = [\"BinaryPrecision()\", \"BinaryRecall()\", \"BinaryF1Score()\"]\n",
    "\n",
    "sr = {}\n",
    "mr = {}\n",
    "for m in metrics:\n",
    "    smean, sstd = get_mean_std(sresults[m])\n",
    "    sscore = str(smean)+\"$_{\\pm\"+str(sstd)+\"}$\"\n",
    "    sr[\"Items\"] = \"Single\"\n",
    "    sr[\"Count\"] = len(single_item_uuids)\n",
    "    sr[m] = sscore\n",
    "\n",
    "    mmean, mstd = get_mean_std(mresults[m])\n",
    "    mscore = str(mmean)+\"$_{\\pm\"+str(mstd)+\"}$\"\n",
    "    mr[\"Items\"] = \"Multi\"\n",
    "    mr[\"Count\"] = len(multi_item_uuids)\n",
    "    mr[m] = mscore\n",
    "\n",
    "print(pd.DataFrame([sr, mr]).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "disambiguator_bm25_preds = load_json(disambiguator_bm25_preds_path)\n",
    "disambiguator_pt_preds = load_json(disambiguator_st_preds_path)\n",
    "qrels_dict = load_json(qrels_path)\n",
    "\n",
    "single_scores = get_scores(qrels_dict, disambiguator_bm25_preds, valid_uuids=single_item_uuids)\n",
    "multi_scores = get_scores(qrels_dict, disambiguator_bm25_preds, valid_uuids=multi_item_uuids)\n",
    "print(\"\\n\")\n",
    "\n",
    "single_scores = get_scores(qrels_dict, disambiguator_pt_preds, valid_uuids=single_item_uuids)\n",
    "multi_scores = get_scores(qrels_dict, disambiguator_pt_preds, valid_uuids=multi_item_uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic-based similarity\n",
    "\n",
    "def extract_variable_basename(variable):\n",
    "    research_id = variable.split(\"_\")[0]\n",
    "    assert \"ZA\" in research_id\n",
    "\n",
    "    basename = variable.split(\"_\")[1:]\n",
    "    return research_id, basename\n",
    "\n",
    "def expand_variable(variable, variable_meta):\n",
    "    research_id, variable_basename = \"\"\n",
    "\n",
    "    return expaned_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = load_jsonl(\"../data/gsim/survey_items.jsonl\")\n",
    "meta_json = {m[\"url\"].split(\":\")[-1]: m[\"variables\"] for m in meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_rdids(variables_list):\n",
    "    return sorted(list(set([_v.split(\"_\")[0] for vs in variables_list for v in vs.split(\";\") for _v in v.split(\",\") if _v and \"ZA\" in _v])))\n",
    "\n",
    "rd_ids = get_clean_rdids(test_df[test_df[\"is_variable\"].isin([\"1\", 1])][\"variable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expandable_vids = []\n",
    "for r in rd_ids:\n",
    "    variables_meta = meta_json.get(r)\n",
    "    for vid in variables_meta:\n",
    "        prefix = f\"exploredata-{r}_\"\n",
    "        vid = vid.split(prefix)[-1]\n",
    "        if \"_\" in vid:\n",
    "            expandable_vids.append(r+\"_\"+vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surrounding_variables(variable, variables_list, n=5):\n",
    "    idx = variables_list.index(variable)\n",
    "    before_start_idx = max(0, idx-n)\n",
    "    after_start_idx = min(idx+1, len(variables_list))\n",
    "    after_end_idx = min(idx+n+1, len(variables_list))\n",
    "    before_n = variables_list[before_start_idx:idx]\n",
    "    after_n = variables_list[after_start_idx:after_end_idx]\n",
    "    return before_n, after_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surrounding_variables(\"exploredata-ZA6670_VarFI_REG\", list(meta_json['ZA6670'].keys()), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_predicted_variables(preds, n_neighbors=2, expand=False, expandable_variables=None):\n",
    "    expanded_preds = {}\n",
    "\n",
    "    for uid, vpreds in preds.items():\n",
    "        expanded_uid_preds = {}\n",
    "        for vid,score in vpreds.items():\n",
    "            expanded_uid_preds[vid] = score\n",
    "            if expandable_variables and vid in expandable_variables or expand:\n",
    "                rd_id = vid.split(\"_\")[0]\n",
    "                assert \"ZA\" in rd_id\n",
    "                pre_vars, post_vars = get_surrounding_variables(\"exploredata-\"+vid, list(meta_json.get(rd_id, []).keys()), n_neighbors)\n",
    "                for ev in pre_vars + post_vars:  # give each expanded variable the same score as the original\n",
    "                    ev = ev.split(\"exploredata-\")[-1]\n",
    "                    if ev not in expanded_uid_preds:\n",
    "                        expanded_uid_preds[ev] = score\n",
    "        expanded_preds[uid] = expanded_uid_preds\n",
    "    return expanded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_disambiguator_pt_preds = expand_predicted_variables({\"abc\": {\"ZA6670_VarNL_INC\": 0.8, \"ZA6670_VarPH_INC\": 0.5}}, expandable_vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_disambiguator_pt_preds = expand_predicted_variables(disambiguator_pt_preds, expandable_vids, n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compare_scores(expanded_disambiguator_pt_preds, disambiguator_pt_preds, qrels_dict, qrels_dict, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(qrels_dict, expanded_disambiguator_pt_preds, metrics=[\"hits@50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(qrels_dict, disambiguator_pt_preds, metrics=[\"hits@50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(qrels_dict, run_dict, valid_uuids=None, metrics=[\"f1@10\", \"precision@10\", \"recall@10\", \"f1@10\", \"map@10\", \"ndcg@10\"]):\n",
    "    if valid_uuids:\n",
    "        qrels_dict = {k:v for k,v in qrels_dict.items() if k in valid_uuids}\n",
    "        run_dict = {k:v for k,v in run_dict.items() if k in valid_uuids}\n",
    "\n",
    "    qrels = Qrels(qrels_dict)\n",
    "    run = Run(run_dict)\n",
    "\n",
    "    results = evaluate(qrels, run, metrics)\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_dict(nested_dict, top_k=10):\n",
    "    new_dict = {}\n",
    "\n",
    "    for k,v in nested_dict.items():\n",
    "        top_vs = list(v.keys())[:top_k]\n",
    "\n",
    "        new_dict[k] = {_k:_v for _k,_v in v.items() if _k in top_vs}\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "def get_recall(qrels, run, top_k=None):\n",
    "    scores = []\n",
    "\n",
    "    for uid, qs in qrels.items():\n",
    "        preds = run.get(uid, {})\n",
    "        preds = list(preds.keys())[:top_k] if top_k else preds\n",
    "        \n",
    "        count = 0\n",
    "        for q in qs:\n",
    "            if q in preds:\n",
    "                count += 1\n",
    "        \n",
    "        score = count / len(qs)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_disambiguator_pt_preds = truncate_dict(disambiguator_pt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recall(qrels_dict, truncated_disambiguator_pt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    expanded_truncated_disambiguator_pt_preds = expand_predicted_variables(truncated_disambiguator_pt_preds, expandable_variables=expandable_vids, n_neighbors=i)\n",
    "    score = get_recall(qrels_dict, expanded_truncated_disambiguator_pt_preds)\n",
    "    print(i+1, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    expanded_truncated_disambiguator_pt_preds = expand_predicted_variables(truncated_disambiguator_pt_preds, n_neighbors=i, expand=True)\n",
    "    score = get_recall(qrels_dict, expanded_truncated_disambiguator_pt_preds)\n",
    "    print(i+1, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../results/data/vadis-prolific-3_project_2023-12-09_1251_12:53:08_train_en.tsv\", sep=\"\\t\")\n",
    "train_df_74709 = train_df[train_df[\"doc_id\"] == 74709]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = train_df_74709[train_df_74709[\"is_variable\"].isin([\"1\", 1])][\"variable\"].tolist()\n",
    "variables = [v for vs in variables for v in vs.split(\";\") if v]\n",
    "variables = [v for vs in variables for v in vs.split(\",\") if v]\n",
    "set(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document-level results\n",
    "doc_var_mapping = {}\n",
    "\n",
    "for doc_id, doc_group in test_df.groupby(by=\"doc_id\"):\n",
    "    variables = doc_group[\"variable\"].fillna(\"\").tolist()\n",
    "    variables = [v for vs in variables for v in vs.split(\";\") if v]\n",
    "    variables = [v for vs in variables for v in vs.split(\",\") if v]\n",
    "    variables = [v for v in variables if \"ZA\" in v]\n",
    "    variables = sorted(set(variables))\n",
    "\n",
    "    doc_var_mapping[str(doc_id)] = variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_doc_var_mapping(df, preds, top_k):\n",
    "    pred_doc_var_mapping = {}\n",
    "\n",
    "    for doc_id, doc_group in df.groupby(\"doc_id\"):\n",
    "        doc_uuids = doc_group['uuid'].tolist()\n",
    "\n",
    "        pred_vars = []\n",
    "        for uid in doc_uuids:\n",
    "            pred = preds.get(uid, {})\n",
    "            \n",
    "            if pred != {}:\n",
    "                pred_vars.extend(list(pred.keys())[:top_k])\n",
    "\n",
    "        pred_vars = sorted(list(set(pred_vars)))\n",
    "        pred_doc_var_mapping[str(doc_id)] = pred_vars\n",
    "\n",
    "    return pred_doc_var_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(true, pred):\n",
    "    count = 0\n",
    "    for p in pred:\n",
    "        if p in true:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_accuracies(test_df, preds):\n",
    "    k_accs = []\n",
    "    k_stds = []\n",
    "    for k in range(1,51):\n",
    "        pred_doc_var_mapping = get_pred_doc_var_mapping(test_df, preds, k)\n",
    "        scores = []\n",
    "\n",
    "        for doc_id,true_vars in doc_var_mapping.items():\n",
    "            if len(true_vars) > 0:\n",
    "                acc = get_accuracy(true_vars, pred_doc_var_mapping[doc_id])\n",
    "                scores.append(acc)\n",
    "\n",
    "        k_accs.append(np.mean(scores)*100)\n",
    "        k_stds.append(np.std(scores)*100)\n",
    "    return k_accs, k_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_accs, gold_stds = get_document_accuracies(test_df, disambiguator_pt_preds)\n",
    "best_accs, best_stds = get_document_accuracies(test_df, disambiguator_pt_preds_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sosse_dataset_size_plot(models, scores, stds, plot_stds=True, output_dir=None, n_columns=3, fontsize=18, figsize=(30,10), y_offset=1, add_title=True):\n",
    "    flat_scores = [s for _s in scores for s in _s]\n",
    "    y_min = max(math.floor(min(flat_scores)-y_offset), 0)\n",
    "    y_max = min(math.ceil(max(flat_scores)+y_offset), 100)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for i,(model,score,std) in enumerate(zip(models, scores, stds)):\n",
    "        score = np.asarray(score)\n",
    "        std = np.asarray(std)\n",
    "\n",
    "        ax.plot(list(range(1,len(score)+1)), score, label=model)\n",
    "        if plot_stds:\n",
    "            ax.fill_between(list(range(1,len(score)+1)), score-std, score+std, alpha=0.2)\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=fontsize)\n",
    "    ax.tick_params(axis='y', labelsize=fontsize)\n",
    "\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel(\"Top k\", fontsize=fontsize)\n",
    "    ax.set_ylabel(\"Recall\", fontsize=fontsize)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, borderaxespad=0.1, loc=\"best\", fancybox=True, framealpha=0.5, fontsize=fontsize).set_title(\"Models\",prop={'size':fontsize})\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, os.path.basename(output_dir)+\".pdf\")\n",
    "        plt.savefig(path, bbox_inches=\"tight\")\n",
    "        print(f\"Save to path: {path}\")\n",
    "        \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_accs, gold_stds = get_document_accuracies(test_df, disambiguator_pt_preds)\n",
    "best_accs, best_stds = get_document_accuracies(test_df, disambiguator_pt_preds_filtered)\n",
    "make_sosse_dataset_size_plot([\"Gold\", \"Best\"], [gold_accs, best_accs], [gold_stds, best_stds], plot_stds=False, output_dir=os.path.join(output_dir, \"document-level-recall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context window results\n",
    "metrics: List[str] = [\"test_lang:en:BinaryPrecision()_mean\", \"test_lang:en:BinaryRecall()_mean\", \"test_lang:en:BinaryF1Score()_mean\", \"test_lang:de:BinaryPrecision()_mean\", \"test_lang:de:BinaryRecall()_mean\", \"test_lang:de:BinaryF1Score()_mean\", \"test_BinaryF1Score()_mean\"]\n",
    "metrics: List[str] = [\"test_type_short:E:BinaryF1Score()_mean\", \"test_type_short:I:BinaryF1Score()_mean\", \"test_subtype_short:Q:BinaryF1Score()_mean\", \"test_subtype_short:P:BinaryF1Score()_mean\"]\n",
    "model_order = [\"SoSci-XLM-R$_{\\text{base}}$\", \"XLM-R$_{\\text{large}}$\"]\n",
    "print(make_table_diagnostic(full_df[full_df[\"result_type\"].isin([\"finetune_contextwindow\"])], metrics, model_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative analysis\n",
    "\n",
    "# Load best models with different architectures and multiple seeds\n",
    "# Predict on validation data\n",
    "# Count the instanaces that most models fail on\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def load_pipeline(path, batch_size=None):\n",
    "    return pipeline(model=path, batch_size=batch_size)\n",
    "\n",
    "val_path = \"../results/data/vadis-prolific-3_project_2023-12-09_1251_12:53:08_val_en.tsv\"\n",
    "val_df = pd.read_csv(val_path, sep=\"\\t\")\n",
    "val_df.index = val_df[\"uuid\"]\n",
    "\n",
    "instance_level_md_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "model_results_dir = \"\"\n",
    "model_dirs = [os.path.join(model_results_dir, d) for d in os.listdir(model_results_dir)]\n",
    "for model_dir in model_dirs:\n",
    "    seed_dirs = [os.path.join(model_dir, d) for d in os.listdir(model_dir)]\n",
    "    model_name = os.path.basename(model_dir)\n",
    "\n",
    "    for seed_dir in seed_dirs:\n",
    "        pipe = load_pipeline(seed_dir, batch_size=8)\n",
    "        uuids = val_df[\"uuid\"].tolist()\n",
    "        texts = val_df[\"sentence\"].tolist()\n",
    "        labels = val_df[\"is_variable\"].tolist()\n",
    "        preds = pipe(texts)\n",
    "\n",
    "        for uid,true,pred in zip(uuids,labels,preds):\n",
    "            instance_level_md_results[uid][model_name].append(1 if true == pred else 0)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show instances with mentions that all models get wrong\n",
    "\n",
    "# show instances with mentions that models sometimes get wrong\n",
    "\n",
    "# show instances without mentions that models get wrong\n",
    "\n",
    "# show instances without mentions that models sometimes get wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_models = [\"multilingual-e5\"]\n",
    "valid_samples = [\"sample=100000\", \"sample=200000\", \"sample=400000\"]\n",
    "top_k = 10\n",
    "instance_level_ed_results = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict())))\n",
    "\n",
    "model_results_dir = \"../results/task2_results_sosci-simlearn/filtered_gen/run_sim_data_size_choice_exp/04-06-2024\"\n",
    "date_dirs = [os.path.join(model_results_dir, d) for d in os.listdir(model_results_dir) if os.path.isdir(os.path.join(model_results_dir, d))]\n",
    "for date_dir in date_dirs:\n",
    "    sample_dirs = [os.path.join(date_dir, d) for d in os.listdir(date_dir) if os.path.isdir(os.path.join(date_dir, d))]\n",
    "\n",
    "    for sample_dir in sample_dirs:\n",
    "        sample = os.path.basename(sample_dir)\n",
    "        seed_dirs = [os.path.join(sample_dir, d) for d in os.listdir(sample_dir) if os.path.isdir(os.path.join(sample_dir, d))]\n",
    "\n",
    "        for seed_dir in seed_dirs:\n",
    "            seed = os.path.basename(seed_dir)\n",
    "            seed_dir = os.path.join(seed_dir, os.listdir(seed_dir)[0])\n",
    "            qrels_path = os.path.join(seed_dir, \"qrels.json\")\n",
    "            run_path = os.path.join(seed_dir, \"run.json\")\n",
    "            results_path = os.path.join(seed_dir, \"results.json\")\n",
    "\n",
    "            qrels = load_json(qrels_path)\n",
    "            run = load_json(run_path)\n",
    "            results = load_json(results_path)\n",
    "\n",
    "            model_path = results[\"config\"][\"model_name\"]\n",
    "            assert sample in model_path\n",
    "            assert seed in model_path\n",
    "            model_name = os.path.basename(model_path).split(\"_epochs=50\")[0]\n",
    "\n",
    "            for vmodel in valid_models:\n",
    "                if vmodel in model_name and sample in valid_samples:\n",
    "                    for uid,entities in qrels.items():\n",
    "                        preds = run.get(uid, {})\n",
    "                        preds = list(preds.keys())[:top_k]\n",
    "\n",
    "                        pred_entities = {}\n",
    "\n",
    "                        for ent in entities:\n",
    "                            label = 0\n",
    "                            if ent in preds:\n",
    "                                label = 1\n",
    "\n",
    "                            pred_entities[ent] = label\n",
    "\n",
    "                        instance_level_ed_results[uid][model_name][sample][seed] = pred_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_path = \"../results/data/vadis-prolific-3_project_2023-12-09_1251_12:53:08_train_en.tsv\"\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "val_path = \"../results/data/vadis-prolific-3_project_2023-12-09_1251_12:53:08_val_en.tsv\"\n",
    "val_df = pd.read_csv(val_path, sep=\"\\t\")\n",
    "eval_df = pd.concat([train_df, val_df])\n",
    "eval_df.index = eval_df[\"uuid\"]\n",
    "\n",
    "aggregated_instance_level_ed_results = {}\n",
    "aggregated_counts = {-1: 0, 0: 0, 1: 0, 10: 0, 51: 0}\n",
    "\n",
    "\"\"\"\n",
    "Aggregates the results of instance-level entity disambiguation (ED) for each variable in the input data.\n",
    "\n",
    "For each instance (identified by a unique ID `uid`), the function processes the ED results for each model, sample, and seed. It counts the occurrences of each value for each variable, and then determines the final value for each variable based on the following rules:\n",
    "\n",
    "- If there are two unique values, the function selects the value with the higher count as the final value. If the counts are equal, it stores the full count dictionary as the final value.\n",
    "- If there is only one unique value, it is stored as the final value.\n",
    "- If there are more than two unique values, the final value is set to -1.\n",
    "\n",
    "The function also keeps track of the counts for the final values of -1, 0, 1, and 51, as well as any other unique values (stored as 10).\n",
    "\n",
    "The final aggregated results for each instance are stored in the `aggregated_instance_level_ed_results` dictionary, keyed by the instance ID.\n",
    "\"\"\"\n",
    "for uid, res in instance_level_ed_results.items():\n",
    "    variable_results = defaultdict(list)\n",
    "\n",
    "    for model, mres in res.items():\n",
    "        for sample, sares in mres.items():\n",
    "            for seed, seres in sares.items():\n",
    "                for vid, vval in seres.items():\n",
    "                    variable_results[vid].append(vval)\n",
    "\n",
    "    aggregated_variable_results = {}\n",
    "    for vid, vvals in variable_results.items():\n",
    "        counts = Counter(vvals)\n",
    "        if len(counts) == 2:\n",
    "            if counts.get(1) > counts.get(0):\n",
    "                aggregated_variable_results[vid] = 51\n",
    "                print(51, vid, eval_df.loc[uid][\"sentence\"])\n",
    "            else:\n",
    "                aggregated_variable_results[vid] = str(dict(counts))\n",
    "        elif len(counts) == 1:\n",
    "            aggregated_variable_results[vid] = vvals[0]\n",
    "            print(vvals[0], vid, eval_df.loc[uid][\"sentence\"])\n",
    "        else:\n",
    "            aggregated_variable_results[vid] = -1\n",
    "\n",
    "        if aggregated_variable_results[vid] in [-1, 0, 1, 51]:\n",
    "            aggregated_counts[aggregated_variable_results[vid]] += 1\n",
    "        else:\n",
    "            aggregated_counts[10] += 1\n",
    "    print(uid, aggregated_variable_results)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    aggregated_instance_level_ed_results[uid] = aggregated_variable_results\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple-experiments-s3mpLMlD-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
